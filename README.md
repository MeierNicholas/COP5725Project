# COP5725Project

## Project Description 
- This is a modified implementation of the AIQL query system. The goal of this project is to allow for effective querying of APT behavior. This project accepts multievent, dependency, and anomaly queries, in the form of AIQL queries, and converts them into SQL queries. These queries are scheduled based on their respective pruning power, and executed on the PostgreSQL database with help from the psycopg Python library. The resulting host/network logs are clearly presented in a table for easy analysis.
- This project is based on the following paper, published at the 2018 USENIX Annual Technical Conference: https://www.usenix.org/system/files/conference/atc18/atc18-gao.pdf

## Dependencies
This project requires several Python libraries in order to function properly. These libraries include:
- psycopg2 (version 2.8.6)
- nltk (version 3.5)
- prettytable (version 2.1.0)
- antlr4 (version 4.9.2)

<em> Note: nltk will require the punkt download. This can be accomplished by spawning a python3 terminal, and entering $import nltk $nltk.download('punkt') </em>

## Included Files
- queryengine.py
	- This is the main project file. This file contains all of the logic to take advantage of ANTLR4's parser, lexer, and listener to analyze AIQL queries and convert them to SQL queries. 
	These queries are then sent to the PostgreSQL database running on the system, using the psycopg library.
- aiql.g4
	- This is our grammar developed for use with ANTLR4. This grammar accepts multievent, dependency, and anomaly queries, as defined in the originally published AIQL paper. Examples of these queries can be found at the bottom of this README.
- The following files are generated as a result of running ANTLR4 on our grammar file, aiql.g4:
	- aiqlLexer.py
		- The lexer is a recognizer that draws input symbols from a character stream. [https://www.antlr.org/api/Java/org/antlr/v4/runtime/Lexer.html]
	- aiqlParser.py
		- The parser recognizes contextual orderings of tokens, as dictated by the parsing rules in the grammar.
	- aiqlListener.py
		- This is a base listener class that we inherit from in our custom Listener class, extendedListener, which is defined within the queryengine.py file.
	- \*.interp
		- The files with the .interp extension are used by IDEs for debugging grammars. These are not necessary for our program.
	- \*.tokens
		- The files with the .tokens extension contain lists of token names and their numeric assignment, which is generated by ANTLR. These files are used by the lexer.
- 250linesHost.csv
	- This is a subset of the network logs used for testing our implementation of this project.
- 250linesNetwork.csv
	- This is a subset of the network logs used for testing our implementation of this project.

## Datasets
- For this project, we took advantage of the open source datasets provided by Los Alamos National Laboratory. Specifically, we used the Unified Host and Network Data Set. This dataset is a subset of network and computer events collected from within the LANL enterprise system over the course of 90 days.
- Host Data
	- The host dataset includes logs collected from Windows machines from within the enterprise network. These logs include the following possible Event IDs: 4768, 4769, 4770, 4774, 4776, 4624, 4625, 4634, 4647, 4648, 4672, 4800, 4801, 4802, 4803, 4688, 4689, 4608, 4609, 1100. 
	- Records will consist of the following possible attributes per event: Time, EventID, LogHost, LogonType, LogonTypeDescription, UserName, DomainName, LogonID, SubjectUserName, SubjectDomainName, SubjectLogonID, Status, Source, ServiceName, Destination, AuthenticationPackage, FailureReason, ProcessName, ProcessID, ParentProcessName, ParentProcessID. The attributes present in each record depends on the EventID.

- Network Data
	- The network events represent bi-directional events when possible. They are in the form of: {Time, Duration, SrcDevice, DstDevice, Protocol, SrcPort, DstPort, SrcPackets, DstPackets, SrcBytes, DstBytes}.

## PostgreSQL Database
- We have one single database, projectdb, which contains two relations. The first relation, hostlogs, contains our host event logs. The screenshot below shows the columns and their respective types in this relation. <br/><br/> ![hostlogs relation](./Assets/hostlogsscreenshot.png)   
The second relation, netlogs, contains our sample network event logs. The screenshot below shows the columns and their repsective types in this relation. <br/><br/> ![nelogs relation](./Assets/netlogsscreenshot.png)

## Accepted Query Types

### Anomaly Queries
These allow the user to query for network anomalies within given time and event constraints.
An example of this query format is as follows:
<em>
	anomaly protocol=17
	(from 118780 to 118785)
	dstpackets>10000
</em>

### Multievent Queries
These allow the user to query for multiple events, specify a temporal relationship between the events, and choose which event to return based on the relationship.
An example of this query format is as follows:
<em>
	domainname=Domain001
	(from 0 to 6000000)
	proc svchost execute proc dllhost.exe as evt1
	proc svchost execute proc wmiprvse.exe as evt2
	with evt1 before evt2
	ret evt1
</em>

### Dependency Queries
These allow the user to query for event paths, where an order is specified using the keywords 'forward' and 'backward'.
An example of this query format is as follows:
<em>
	(from 18723 to 18900)
	backward: proc p1 -> [execute] file wmiprvse.exe <- [end] proc p2 <- [priv] proc p3 ret p3
</em>

For more information on the structure and function of these queries, please read our paper at [An Implementation of a Query System for Investigating Complex Attack Behaviors for Enterprise Security](www.google.com)

## Running the System (assuming all dependencies are met and the database has been initialized with the proper relations)
1. Run 'python3 queryengine.py'
2. Enter an anomaly, multievent, or dependency query. The system currently expects queries to be properly formatted per the examples above.
3. The results of the query will then be printed to the screen.

<em> Note: If you are interested in seeing the parse tree produced by our grammar given a specific query, please reference the [getting started with Antlr4](https://github.com/antlr/antlr4/blob/master/doc/getting-started.md) documentation. </em>
